#---
#layout: post
#title:  "[NLP 기초] Part1"
#date:   2019-04-23
#category: first-contact
#tags: first-contact
#author: Khel Kim, 김현호
#comments: true
#---
<br><br>
NLP 기초
[참고 사이트](https://wikidocs.net/book/2155)
<br><br>
# 0. 자연어 처리(Natural Language Processing)란?

자연어(Natural Language)란 우리가 일상 생활 속에서 사용하는 언어를 말합니다. 자연어 처리(Natural Language Processing)란 이러한 자연어의 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 일입니다.  


자연어 처리는 음성 인식, 내용 요약, 번역, 사용자의 감정 분석, 텍스트 분류 작업(스팸 메일 분류, 뉴스 기사 카테고리 분류), 질의 응답 시스템, 챗봇과 같은 곳에서 사용되는 연구 분야입니다.  

이 포스팅에서는 자연어 처리에 필요한 1. 텍스트 전처리 방법 2. 딥 러닝 이전에 주류로 사용되었던 통계 기반의 언어 모델에 대해 다루겠습니다.
<br>

# 1. 텍스트 전처리(Text Preprocessing)

## 1. 토큰화(Tokenization)

자연어 처리에서 자주 쓰는 용어 중 코퍼스라는 용어가 있습니다. 코퍼스(corpus)란 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합입니다. 다른 말로는 말뭉치라고도 합니다. 우리가 분석해야하는 데이터이며, 분석의 정확성을 위해 해당 자연언어를 형태소 분석하는 경우가 많습니다. 확률/통계적 기법과 시계열적인 접근으로 전체를 파악합니다.


자연어 처리에서 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 정제되지 않은 상태라면, 해당 데이터를 사용하고자하는 용도에 맞게 토큰화(Tokenization)(특정 단위로 쪼개는 일)와 정제(Normalization) 하는 일을 하게 됩니다. 우리는 이 중 토큰화를 먼저 살펴보겠습니다.


토큰(Token)은 목적을 위해 분류화를 명시적으로 지시하는 어휘소를 표현하는 구조의 하나입니다. 주어진 코퍼스를 토큰(Token)는 단위로 나누는 작업을 우리는 토큰화(Tokenization)라고 부릅니다. 토큰의 단위는 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의한다고 보면 좋습니다. 예를 들어, 형태소 단위, 단어 단위, 문장 단위등이 있습니다.


### 1. 단어 토큰화(Word Tokenization)

토큰의 기준을 단어(Word)로 하는 경우, 단어 토큰화(Word Tokenization)라고 합니다. 여기서 단어는 단어 외에도 단어구, 의미를 갖는 문자열로도 간주됩니다.

~~~
import nltk
from nltk.tokenize import word_tokenize

word_tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.")
~~~
~~~
## result
['Do',
 "n't",
 'be',
 'fooled',
 'by',
 'the',
 'dark',
 'sounding',
 'name',
 ',',
 'Mr.',
 'Jone',
 "'s",
 'Orphanage',
 'is',
 'as',
 'cheery',
 'as',
 'cheery',
 'goes',
 'for',
 'a',
 'pastry',
 'shop',
 '.']
~~~
word_tokenize는 Don't를 Do와 n't로 분리하였으며, 반면 Jone's는 Jone과 's로 분리합니다.
~~~
from nltk.tokenize import WordPunctTokenizer

WordPunctTokenizer().tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.")
~~~
~~~
## result
['Don',
 "'",
 't',
 'be',
 'fooled',
 'by',
 'the',
 'dark',
 'sounding',
 'name',
 ',',
 'Mr',
 '.',
 'Jone',
 "'",
 's',
 'Orphanage',
 'is',
 'as',
 'cheery',
 'as',
 'cheery',
 'goes',
 'for',
 'a',
 'pastry',
 'shop',
 '.']
~~~
WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 갖고 있기때문에, 앞서 확인했던 word_tokenize와는 달리 Don't를 Don과 '와 t로 분리하였습니다. 이와 마찬가지로 Jone's를 Jone과 '와 s로 분리한 것을 확인할 수 있습니다.

이처럼 모듈별로 토큰화를 다르게 진행할 수 있습니다. 따라서 저희는 필요와 목적에 맞게 모듈을 잘 골라서 사용해야 합니다.

### 2. 토큰화에서 고려해야할 사항

1. 구두점이나 특수 문자를 단순 제외해서는 안 된다.  
    예를 들어, Ph.D나 01/02/06과 같이 의미를 같는 특수문자의 경우 함부로 제외해서는 안된다.  
    참고: 미리 정의해 둔 사람, 회사, 장소, 시간, 단위 등에 해당하는 단어(개체명, Named-Entity)를 인식하여 추출 분류하는 기법을 개체명 인식(NER, Named-Entity, Recognition)이라고 합니다.

2. 줄임말과 단어 내에 띄어쓰기가 있는 경우.  
    예를 들어, we're같은 경우(re를 접어(clitic)이라고 함) '는 압축된 단어를 다시 펼치는 역활을 합니다. 또, New York 같은 경우 하나의 단어지만 중간에 띄어쓰기가 존재합니다.


- Tokenization 예제  
예제로서 표준으로 쓰이고 있는 Tokenization 중 하나인 Penn Treebank Tokenization을 살펴보도록 하겠습니다. Tokenization은 두가지 규칙을 따릅니다.  
규칙1. 하이푼으로 구성된 단어는 하나로 유지한다.  
규칙2. doesn't와 같이 접어가 함께하는 단어는 분리해준다.
~~~
from nltk.tokenize import TreebankWordTokenizer

tokenizer = TreebankWordTokenizer()
text = "Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own. New York"

tokenizer.tokenize(text)
~~~
~~~
## result
['Starting',
 'a',
 'home-based',
 'restaurant',
 'may',
 'be',
 'an',
 'ideal.',
 'it',
 'does',
 "n't",
 'have',
 'a',
 'food',
 'chain',
 'or',
 'restaurant',
 'of',
 'their',
 'own.',
 'New',
 'York']
~~~


### 3. 문장 토큰화(Sentence Tokenization)  

이 작업은 토큰의 단위가 문장(Sentence)일 때 주어진 코퍼스를 문장 단위로 구분하는 작업입니다. 때로는 문장 분류(Sentence Segmentation)라고도 불립니다.

~~~
from nltk.tokenize import sent_tokenize

text = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near."

sent_tokenize(text)
~~~
~~~
## result
['His barber kept his word.',
 'But keeping such a huge secret to himself was driving him crazy.',
 'Finally, the barber went up a mountain and almost to the edge of a cliff.',
 'He dug a hole in the midst of some reeds.',
 'He looked about, to mae sure no one was near.']
~~~
nltk는 단순히 온점을 구분자로 하여 문장을 구분하지 않기 때문에 문장 내의 온점을 단어로 인식할 수 있습니다.
~~~
text = "I am actively looking for Ph.D. students. and you are a Ph.D student."

sent_tokenize(text)
~~~
~~~
## result
['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']
~~~

### 4. 한국어에서의 토큰화의 어려움

1. 조사가 띄어쓰기 없이 바로 붙는다.
2. 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.


### 5. 품사 부착(Part-of-speech tagging)

단어는 표기는 같지만, 품사에 따라서 단어의 의미가 달라지기도 합니다. 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 보는 것이 중요합니다. 그에 따라 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지 구분해놓기도 하는데, 이 작업을 품사 부착(Part-of-speech taggning)이라고 합니다.

~~~
text = "I am actively looking for Ph.D. students. and you are a Ph.D. student."

from nltk.tag import pos_tag

tmp = word_tokenize(text)
pos_tag(tmp)
~~~
~~~
## result
[('I', 'PRP'),
 ('am', 'VBP'),
 ('actively', 'RB'),
 ('looking', 'VBG'),
 ('for', 'IN'),
 ('Ph.D.', 'NNP'),
 ('students', 'NNS'),
 ('.', '.'),
 ('and', 'CC'),
 ('you', 'PRP'),
 ('are', 'VBP'),
 ('a', 'DT'),
 ('Ph.D.', 'NNP'),
 ('student', 'NN'),
 ('.', '.')]
~~~

~~~
from konlpy.tag import Okt ## Open Korea Text

okt = Okt()
text = "열심히 코딩한 당신, 연휴에는 여행을 가봐요"

okt.morphs(text)
~~~
~~~
## result
['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']
~~~

~~~
okt.pos(text)
~~~
~~~
## result
[('열심히', 'Adverb'),
 ('코딩', 'Noun'),
 ('한', 'Josa'),
 ('당신', 'Noun'),
 (',', 'Punctuation'),
 ('연휴', 'Noun'),
 ('에는', 'Josa'),
 ('여행', 'Noun'),
 ('을', 'Josa'),
 ('가봐요', 'Verb')]
~~~

1. morphs: 형태소 추출
2. pos: 품사 부착(Part-of-speech tagging)
3. nouns: 명사 추출


## 2. 정제(Normalization)

단어의 정제 작업의 목적은 크게 두 가지로 생각해볼 수 있습니다.
1. 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.
2. 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.  
    노이즈를 제거하는 정제 작업은 토큰화 작업에 방해가 되는 부분들을 배제시키고 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지기도 하지만, 토큰화 작업 이후에도 여전히 남아있는 노이즈들을 제가하기위해 지속적으로 이루어지기도 합니다.


### 1. 규칙에 기반한 표기가 다른 단어들의 통합
1. 어간 추출(Stemming)
2. 표제어 추출(Lemmatization)  
을 이용해 통합할 수 있습니다. 자세한건 다른 장에서 다루겠습니다.


### 2. 대, 소문자 통합
가끔 예외 상황이 있지만, 모든 코퍼스를 소문자로 바꾸는 것이 실용적인 해결책이 될 수 있습니다.
### 3. 불필요한 단어의 제거  

1. 불용어 제거(is, a, the, etc.)
2. 등장 빈도가 적은 단어
3. 길이가 너무 짧거나 긴 단어


### 4. 정규 표현식

얻어낸 코퍼스에는 코퍼스의 특징에 따라서 특정 규칙이 있는 경우가 많습니다. 코퍼스내에 계속해서 등장하는 글자들을 한 번에  제거하는 방식으로서 정규 표현식은 유용합니다. 정규 표현식은 다른 장에서 다루겠습니다.


## 3. 어간 추출(Stemming)과 표제어 추출(Lemmatization)

어간 추출(Stemming)과 표제어 추출(Lemmatization)은 코퍼스의 단어의 개수를 줄일 수 있는 기법입니다.

### 1. 표제어 추출(Lemmatization)

표제어 추출은 단어들이 서로 다른 모습을 갖고 있더라도, 그 뿌리의 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단합니다. 예를 들어, am, are, is의 뿌리는 be입니다.


~~~
from nltk.stem import WordNetLemmatizer

n=WordNetLemmatizer()
words = ['policy', 'doing', 'organization',
         'have', 'going', 'love', 'lives',
         'fly', 'dies', 'watched', 'has', 'starting']

[n.lemmatize(w) for w in words]
~~~
~~~
## result
['policy',
 'doing',
 'organization',
 'have',
 'going',
 'love',
 'life',
 'fly',
 'dy',
 'watched',
 'ha',
 'starting']
~~~

표준어 추출은 단어의 형태가 적절히 보존되는 양상을 보이는 특징이 있습니다. 그럼에도 불구하고 위의 결과에서는 dy나 ha와 같이 의미를 알 수 없는 적절하지 못한 단어를 출력하고 있습니다. 이는 표제어 추출기(Lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문입니다.

~~~
n.lemmatize('dies', 'v')
~~~
~~~
## result
'die'
~~~


### 2. 어간 추출(Stemming)

형태소는 의미를 가진 가장 작은 단위를 뜻합니다. 형태소는 두 가지 종류가 있습니다.
1. stem(어간)  
    단어의 의미를 담고있는 단어의 핵심 부분
2. affix(접사)  
    단어의 추가적인 의미를 주는 부분


어간(stem)을 추출하는 작업을 stemming이라고 합니다. 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있습니다.

~~~
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

s = PorterStemmer()
text = "This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."
words = word_tokenize(text)

print(words)
[s.stem(w) for w in words]
~~~
~~~
## result
['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', "'s", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']

['thi',
 'wa',
 'not',
 'the',
 'map',
 'we',
 'found',
 'in',
 'billi',
 'bone',
 "'s",
 'chest',
 ',',
 'but',
 'an',
 'accur',
 'copi',
 ',',
 'complet',
 'in',
 'all',
 'thing',
 '--',
 'name',
 'and',
 'height',
 'and',
 'sound',
 '--',
 'with',
 'the',
 'singl',
 'except',
 'of',
 'the',
 'red',
 'cross',
 'and',
 'the',
 'written',
 'note',
 '.']
~~~
~~~
from nltk.stem import LancasterStemmer

l = LancasterStemmer()
[l.stem(w) for w in words]
~~~
~~~
## result
['thi',
 'was',
 'not',
 'the',
 'map',
 'we',
 'found',
 'in',
 'bil',
 'bon',
 "'s",
 'chest',
 ',',
 'but',
 'an',
 'acc',
 'cop',
 ',',
 'complet',
 'in',
 'al',
 'thing',
 '--',
 'nam',
 'and',
 'height',
 'and',
 'sound',
 '--',
 'with',
 'the',
 'singl',
 'exceiv',
 'of',
 'the',
 'red',
 'cross',
 'and',
 'the',
 'writ',
 'not',
 '.']
~~~

두 스태머 알고리즘은 서로 다른 알고리즘을 사용하기 때문에 사용하고자 하는 코퍼스에 스태머를 적용해보고 어떤 스태머가 해당 코퍼스에 적합한지를 판단한 후에 사용하여야 합니다.  


이런 규칙에기반한 알고리즘은 가끔 제대로 된 일반화를 수행하지 못 할 수도 있습니다. 어간 추출을 하고나서 일반화가 지나치게 되거나, 또는 덜 되거나 하는 경우입니다.


## 4. 불용어(Stopword)

### 1. nltk를 통해서 불용어 제거하기
갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요합니다. 여기서 큰 의미가 없다는 것은 문장 내에서는 자주 등장하지만 문장을 분석하는 데 있어서는 큰 도움이 되지 않는 단어들을 말하는 것입니다.   

nltk에서는 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있습니다.

~~~
from nltk.corpus import stopwords

stopwords.words('english.text')[:10]
## 저는 확장자를 바꿔야 실행됐습니다.
~~~
~~~
## result
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]
~~~


~~~
text = "Family is not an important thing. It's everything."
stop_words = set(stopwords.words("english.text"))

word_tokens = word_tokenize(text)

print(word_tokens)
[word for word in word_tokens if word not in stop_words]
~~~
~~~
## result
['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
['Family', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
~~~


### 2. 한국어에서 불용어 제거하기
~~~
text = "고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지."
stop_words = '아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든'
stop_words = stop_words.split(' ')

word_tokens = word_tokenize(text)

print(word_tokens)
print([word for word in word_tokens if word not in stop_words])
~~~
~~~
## result
['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']
['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']
~~~
## 5. 정규 표현식(Regular Expression)

정규 표현식은 실습으로 익히도록 하겠습니다.
1.
~~~
import re

r = re.compile('a.c')
r.search('kkk')
~~~
~~~
r.search('abc')
~~~
~~~
## result
<_sre.SRE_Match object; span=(0, 3), match='abc'>
~~~
2.
~~~
r = re.compile("ab?c")
r.search('abbc')
~~~
~~~
r.search('abc')
~~~
~~~
## result
<_sre.SRE_Match object; span=(0, 3), match='abc'>
~~~
~~~
r.search('ac')
~~~
~~~
## result
<_sre.SRE_Match object; span=(0, 2), match='ac'>
~~~
3.
~~~
r = re.compile("ab*c")
r.search('a')
~~~
~~~
r.search('ac')
~~~
~~~
## result
<_sre.SRE_Match object; span=(0, 2), match='ac'>
~~~
~~~
r.search('abbbc')
~~~
~~~
## result
<_sre.SRE_Match object; span=(0, 5), match='abbbc'>
~~~
4.
~~~
r = re.compile('ab+c')
r.search('ac')
~~~
~~~
r.search('abc')
~~~
~~~
## result
<_sre.SRE_Match object; span=(0, 3), match='abc'>
~~~
~~~
r.search('abbc')
~~~
~~~
## result
<_sre.SRE_Match object; span=(0, 4), match='abbc'>
~~~
5.
~~~
r = re.compile('^a')
r.search('bbc')
~~~
~~~
r.search('ab')
~~~
~~~
## result
<_sre.SRE_Match object; span=(0, 1), match='a'>
~~~
6.
~~~
r = re.compile('ab{2}c')
r.search('abc')
~~~
~~~
r.search('abbc')
~~~
~~~
## result
<_sre.SRE_Match object; span=(0, 4), match='abbc'>
~~~
~~~
r.search('abbbc')
~~~
7.
~~~

~~~
~~~
## result

~~~
8.
~~~

~~~
~~~
## result

~~~
9.
~~~

~~~
~~~
## result

~~~

~~~

~~~
~~~
## result

~~~

~~~

~~~
~~~
## result

~~~

~~~

~~~
~~~
## result

~~~

~~~

~~~
~~~
## result

~~~
